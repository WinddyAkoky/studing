{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 随机森林\n",
    "\n",
    "集成学习有两个流派：\n",
    "- Boosting: 各个弱学习器之间有依赖关系\n",
    "- Bagging：各个弱学习之间没有依赖关系\n",
    "\n",
    "![](./bagging.png)\n",
    "\n",
    "**Bagging一个特点是：随机采样，且是有放回采样**\n",
    "\n",
    "假设数据集样本数量为$m$，对于每一个样本，每次采样是被采样到的概率为$\\frac{1}{m}$。则$m$次采样都没有猜中的概率为$(1-\\frac{1}{m})^{m}$，当$m->\\infty$时，$(1-\\frac{1}{m})^{m} -> \\frac{1}{e} \\approx 0.368$。也就是说，在bagging每轮随机采样中，训练集中大约有$36.8%$的数据没有被采样采到。\n",
    "\n",
    "对于这部分大约$36.8%$没有被采样到的数据，我们常常称之为**袋外数据**。这些数据没有参与训练集模型的拟合，因此可以用来检测模型的泛化能力。\n",
    "\n",
    "- Bagging对于若学习器没有限制，这和Adaboost一样。\n",
    "- Bagging的集合策略也比较简单，对于分类问题，通常使用简单投票法，得到最多票数的类别或者类别之一为最终的模型输出。对于回归问题，通常使用简单平均法，对于T个弱学习器得到的回归结果进行算术平均得到最终的模型输出。\n",
    "- Bagging算法每次都进行采样来训练模型，因此泛化能力很强，对于降低模型的方差很有作用。当然，对于训练集的拟合程度就会差一点，也就是模型的偏差会大一些。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 随机森林算法\n",
    "\n",
    "- RF使用了CART决策树作为弱学习器。\n",
    "- 在使用决策树的基础上，RF对决策树的建立做了改进，对于普通的决策树，我们会在节点上所有的n个样本特征中选择一个最优的特征来做决策树的左右子树划分，但是RF通过随机选择节点上的一部分样本特征，然后选择一个最优的特征来做决策树的左右子树划分。这样进一步增强了模型的泛化能力。\n",
    "\n",
    "### 随机森林总结\n",
    "\n",
    "随机森林的优点：\n",
    "- 训练可以并行化。\n",
    "- 由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型。\n",
    "- 在训练后，可以给出各个特征对于输出的重要性。\n",
    "- 由于采用随机采样，训练出的模型的方差小，泛化能力强。\n",
    "- 实现简单\n",
    "- 对异常值不敏感\n",
    "\n",
    "缺点：\n",
    "- 难以学习到组合特征\n",
    "- 在每颗决策树的生成过程中，每一次划分都是做出一次局部最优的选择，最终结果并不能保证全局最优"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}
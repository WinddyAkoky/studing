{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# FM\n",
    "\n",
    "FM: Feature Machine\n",
    "\n",
    "- FM解决的问题：大规模稀疏数据下的特征组合问题。\n",
    "\n",
    "\n",
    "1. 为什么要特征组合：\n",
    "\n",
    "实践中通过观察大量的样本数据可以发现，某些特征经过关联之后，与label之间的相关性就会提高。例如“USA”与“THANKSGIVING”, \"CHINA\"与“Chinese New Year\"这样的关联特征，对用户的点击有着正向的影响。\n",
    "\n",
    "2. 如何组合：\n",
    "\n",
    "数学模型上表达特征$x_i$与$x_j$组合用$x_ix_j$表示，即所有的多项式模型， 通常情况下只考虑二阶多项式模型，也就是特征两两组合的问题。二阶多项式的模型如下：\n",
    "\n",
    "$$y = w_0+\\sum_{i=1}^n w_ix_i+\\sum_{i=1}^{n-1}{\\sum_{j=i+1}^n{w_{ij}x_ix_j}}$$\n",
    "\n",
    "其中n代表样本的特征数量，这里的特征是离散化后的特征，$x_i$是第i个特征的值，$w_0,w_i,W_{ij}$是模型参数。从公式来看，模型前半部分就是普通的LR线性组合，后半部分的交叉项就是特征的组合。\n",
    "\n",
    "3. 如何求解二次项参数 $w_{ij}$\n",
    "\n",
    "上面模型有个潜在问题：它对组合特征建模，泛化能力比较弱，尤其是在大规模稀疏特征存在的场景下，这个毛病尤其突出，比如CTR预估和推荐排序。\n",
    "\n",
    "4. FM模型的提出\n",
    "\n",
    "$$FM: \\hat{y}(x)=w_0+\\sum_{i=1}^n{w_ix_i}+\\sum_{i=1}^n{\\sum_{j=i+1}^n{<v_i,v_j>x_ix_j}}$$\n",
    "\n",
    "$$<v_i,v_j>=\\sum_{f=1}^k{v_{i,j} \\cdot v_{j,v}}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- FM模型也直接引入任意两个特征的二阶特征组合，和SVM模型最大的不同，在于特征组合的权重的计算方法。\n",
    "- FM对于每个特征，学习一个大小为k的以为向量，于是，两个特征$x_i$和$x_j$的特征组合的权重值，通过特征对应的向量$v_i$和$v_j$的內积$<v_i,v_j>$来表示。\n",
    "- 这本质上是对特征进行embedding化表征，和目前非常常见的各种实体embedding 本质思想一致的。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "5. 为什么说FM的这种特征embedding模型，在大规模稀疏特征引用环境下比较好用？为什么它的泛化能力强呢？\n",
    "\n",
    "比如，在训练数据里两个特征并未同时在训练实例里见到过，意味着$x_i and x_j$ 一起出现过的次数为0，如果是SVM的模型，是无法学会这个特征组合的权重的。但是因为FM是学习单个特征的embedding，并不依赖于某个特征组合是否出现过，所以只要特征$x_i$和其他任意特征组合没有看到过，但是在预测的时候，如果看到这个新的特征组合，因为$x_i$和$x_j$都能学会自己对应的embedding，所以可以通过內积算出这个新特征组合的权重。这是为何说FM模型泛化能力强的根本原因。\n",
    "\n",
    "6. 改写FM公式\n",
    "\n",
    "$$\\sum_{i=1}^n{\\sum_{j=i+1}^n{<v_i,v_j>x_ix_j}}$$\n",
    "$$=\\frac{1}{2}\\sum_{i=1}^n{\\sum_{j=1}^n{<v_i,v_j>x_ix_j}}-\\frac{1}{2}\\sum_{i=1}^n{<v_i,v_j>x_ix_i}$$\n",
    "$$=\\frac{1}{2}(\\sum_{i=1}^n{\\sum_{j=1}^n{\\sum_{f=1}^k{v_{i,f}v_{j,f}x_ix_j}}} - \\sum_{i=1}^n{\\sum_{f=1}^k{v_{i,f}v_{j,f}x_ix_i}})$$\n",
    "$$=\\frac{1}{2}\\sum_{f=1}^k{((\\sum_{i=1}^n{v_{i,f}x_i})(\\sum_{j=1}^n{v_{j,f}x_j})-\\sum_{i=1}^n{v_{i,f}^2x_i^2)}}$$\n",
    "$$=\\frac{1}{2}\\sum_{f=1}^k{((\\sum_{i=1}^n{v_{i,f}x_i})^2 - \\sum_{i=1}^n{v_{i,f}^2x_i^2})}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# FFM \n",
    "\n",
    "FFM：Field-aware factorization machines\n",
    "\n",
    "- 相比于FM模型，FFM引入了field的概念，特征可以被归类到field中。\n",
    "\n",
    "https://www.biaodianfu.com/ctr-fm-ffm-deepfm.html"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "1. 一个例子：ESPN、Vogue和NBC同属于出版商，Nike、Gucci和Adidas属于fieId广告商。\n",
    "\n",
    "| Clicked | Publisher(P) | Advertiser(A) | Gender(G) |\n",
    "|---------|--------------|---------------|-----------|\n",
    "| Yes     | ESPN         | Nike          | Male      |\n",
    "\n",
    "\n",
    "在FM模型中，FM的信息包含：$w_{ESPN}\\cdot w_{Nike}+w_{ESPN}\\cdot w_{Male}+w_{Nike}\\cdot w_{Male}$, ESPN会和Nike与Male的权重进行计算，但Nike和Male属于不同的field，所以他们的潜在影响可能会不同。\n",
    "\n",
    "FFM加入field的概念，对于刚才的例子来说，二次项变为：\n",
    "$$w_{ESPN,A}\\cdot w_{Nike,P}+w_{ESPN,G}\\cdot w_{Male,P}+w_{Nike,G}\\cdot w_{Male,A}$$\n",
    "\n",
    "- 对于特征组合（EPSN，Nike）来说，其隐变量采用的是$w_{EPSN,A}$和$w_{Nike,P}$，对于$w_{EPSN,A}$这是因为Nike属于广告主（Advertiser）的field，而第二项$w_{Nike,P}$则是EPSN是广告商（Publisher）的field。\n",
    "- 对于特征组合（EPSN，Male）来说，$w_{EPSN,G}$是因为Male是用户性别（Gender）的field，而第二项$w_{Male,P}$是因为EPSN是广告商（Publisher）的field。\n",
    "\n",
    "2. 因此，FFM的数学公式表示为：\n",
    "\n",
    "$$y(x)=w_0+\\sum_{i=1}^d{w_ix_i}+\\sum_{i+1}^d{\\sum_{j=i+1}^d{(w_{i,f_j}\\cdot w_{j,f_i})x_ix_j}}$$\n",
    "\n",
    "$f_i$和$f_j$分别代表第i个特征和第j个特征所属的field。若field有f个，隐向量的长度为k，则二次项的系数共有$dfk$个，远多于FM模型的$dk$个。此外，隐向量和field相关，并不能像FM模型一样将二次项简化，计算的复杂度是$d^2k$。\n",
    "\n",
    "通常情况下，每个隐向量只需要学习特定的field的表示，所以有$k_{FFM}<<k_{FM}$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## FFM模型学习\n",
    "\n",
    "为了方便推导，这里省略FFM的一次项和常数项，公式为：\n",
    "\n",
    "$$\\phi(w,x)=\\sum_{a=1}^d\\sum_{b=a+1}^d{(w_{a,f_b}w_{b,f_a})x_a,x_b}$$\n",
    "\n",
    "FFM模型使用logistic loss作为损失函数，并加上L2正则化：\n",
    "\n",
    "$$L = \\sum_{i=1}^N{log(1+exp(-y_i\\phi(w,x_i))) + \\frac{\\lambda}{2}\\|w\\|^2}$$\n",
    "\n",
    "采用随机梯度下降来(SGD)来优化损失函数，因此，损失函数只采用单个样本的损失：\n",
    "\n",
    "$$L=log(1+exp(-y_i\\phi(w,x_i))) + \\frac{\\lambda}{2}\\|w\\|^2$$\n",
    "\n",
    "对于每次迭代，选取一条数据(x,y)，然后对$w_{a,f_b}$和$w_{b,f_a}$求偏导：\n",
    "\n",
    "$$g_{a,f_b}=\\frac{\\partial L}{\\partial w_{a,f_b}}=\\kappa\\cdot w_{b,f_a}x_ax_b+\\lambda w_{a,f_b}$$\n",
    "$$g_{b,f_a}=\\frac{\\partial L}{\\partial w_{b,f_a}}=\\kappa\\cdot w_{a,f_b}x_ax_b+\\lambda w_{b,f_a}$$\n",
    "\n",
    "其中$\\kappa = \\frac{-y}{1+exp(y\\phi(w,x))}$\n",
    "\n",
    "设$g_{t,j}$是第t轮第j个参数的梯度，则SGD和采用Adagrad的参数更新公式分别如下：\n",
    "\n",
    "$$SGD: w_{t+1,j}=w_{t,j}-\\eta \\cdot g_{t,j}$$\n",
    "$$Adagrad: w_{t+1,j}=w_{t,j}-\\frac{\\eta}{\\sqrt{G_{t,jj}+\\epsilon}}\\cdot g_{t,j}$$\n",
    "\n",
    "Adagrad在学习率$\\eta$上还除以一项$\\sqrt{G_{t,jj}+\\epsilon}$. $\\epsilon$是平滑项，防止分母为0. $G_{t,jj}=\\sum_{l=1}^tg_{l,jj}^2$即为$G_{t,jj}$的对角矩阵，每个对角线位置的值为参数$w_j$每一轮的平方和，可以看出，随着迭代的进行，每个参数的历史梯度累加到一起，使得每个参数的学习率逐渐减少。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- 要使用FFM模型，特征需要转化为“field_id:feature_id:value”格式，相比LibSVM的格式多了field_id，即特征所属的field的编号，feature_id是特征编号，value为特征的值\n",
    "- FFM模型对于one-hot后类别特征十分有效，但是如果数据不够稀疏，可能相比其它模型提升没有稀疏的时候那么大，此外，对于数值型的数据效果不是特别的好"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![](./FM.png)\n",
    "![](./FM-DNN.png)\n",
    "![](./DeepFM.png)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "DeepFM的架构：\n",
    "\n",
    "1. 输入的是稀疏特征的id\n",
    "2. 进行一层lookup之后得到id的稠密embedding\n",
    "3. 这个embedding一方面作为隐向量输入到FM层进行计算\n",
    "4. 同时该embedding进行聚合之后输入到一个DNN模型\n",
    "5. 然后将FM层和DNN层的输入求和之后进行co-train\n",
    "\n",
    "- DeepFM目的是同时学习低阶和高阶的特征交叉，主要由FM和DNN两部分组成，底部共享同样的输入。模型可以表示为：\n",
    "\n",
    "$$\\hat{y}=sigmoid(y_{FM}+y_{DNN})$$\n",
    "\n",
    "### FM部分\n",
    "\n",
    "$$y_{FM}=<w,x> + \\sum_{i=1}^d\\sum_{j=i+1}^d<V_i,V_j>x_i\\cdot x_j$$\n",
    "\n",
    "### Deep部分\n",
    "\n",
    "- 深度部分是一个前馈神经网络，与图像或语音类的输入不同，CTR的输入一般是极其稀疏的，因此需要重新设计网络结构。在第一层隐藏层之前，引入一个嵌入层来完成输入向量压缩到低位稠密向量\n",
    "- 尽管不同field的输入长度不同，但是embedding之后向量的长度均为k\n",
    "- 在FM中得到的隐变量$V_{ik}$现在作为嵌入层网络的权重\n",
    "\n",
    "![](./DeepFM-2.png)\n",
    "\n",
    "\n",
    "- 嵌入层的输出为$a^{(0)}=[e_1,e_2,...,e_m]$，其中$e_i$是嵌入层的第i个field，m是field的个数，前向过程将嵌入层的输出输入到隐藏层为：\n",
    "$$a^{l+1}=\\sigma(W^{l}a^{l}+b^{l})$$\n",
    "- 其中l是层数，σ是激活函数，W(l)是模型的权重，b(l)是l层的偏置，因此，DNN得预测模型表达为：\n",
    "$$y_{DNN}=W^{|H|+1} \\cdot a^{|H|}+b^{|H|+1}$$\n",
    "其中，|H|为隐藏层层数。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "https://www.biaodianfu.com/ctr-fm-ffm-deepfm.html\n",
    "https://blog.csdn.net/wyfiveron/article/details/106318494"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}
{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# XGBoost\n",
    "\n",
    "XGBoost (extreme gradient boosting) 极致梯度提升，是一种基于GBDT的算法或者说工程实现。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "XGBoost的基本思想和GBDT相同，但是做了一些优化，比如二阶导数使函数更精准；正则化避免树过拟合；Block存储可以并行计算等。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. XGBoost目标函数推导\n",
    "\n",
    "XGBoost的目标函数由损失函数和正则化两部分组成。\n",
    "\n",
    "已知训练数据集$T=\\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\}$，损失函数$l(y_i,\\hat(y)_i)$，正则化项$\\Omega(f_k)$，则整体目标函数可记为：\n",
    "$$L(\\phi)=\\sum_i{l(y_i,\\hat{y}_i)} + \\sum_k{\\Omega(f_k)}$$\n",
    "\n",
    "其中：\n",
    "- $L(\\phi)$是线性空间上的表达\n",
    "- i是第i个样本，k是第k颗数。\n",
    "- $\\hat{y}_i$是第i个样本$x_i$的预测值\n",
    "$$\\hat{y}_i = \\sum_{k=1}^K{f_k(x_i)}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}